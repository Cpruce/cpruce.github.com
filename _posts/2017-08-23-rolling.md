---
layout: post
title: "Rolling Average & Maximum"
tagline: "Windows Without Any Panes"
image: clouds 
author: "Cory Pruce"
---

![Rolling Graph](/assets/rolling_graph.png){: .center-image }

# [View Project](https://github.com/Cpruce/Notebooks/blob/master/Rolling Average and Maximum.ipynb)

<!--
# Why Study Old Ideas in New Approaches?

- Obvious: new perspective could be uncorrelated enough with the X previous perspectives such that a new idea is sprouted. This is similar to ensembling multiple predictive models together in order to correct any incorrect generalizations made by a single model, which is called reducing the variance error. In a nutshell, N minds are better than 1.
- Not-so Obvious: faster iteration times facilitates a higher learning/thinking rate, thus increasing the likelihood of discovering something novel. Relating to another idea in data science/machine learning today, GPU's are incredibly vital to making progress on a model and/or problem. The faster one gets feedback on model performance, the faster one can try new experiments and a larger parameter space can be tried.

In a sense, both of these can be combined under one concept: improvement. 

## Streams are Important

In reality, data sets are simply fixed-sized streams. Tools such as Spark and Flink leverage this notion of an iterator, fixed or infinite. 

The interesting part of data streams is what can be applied on the stream, or a partition of the stream. For example, online machine learning algorithms and temporal analysis can be applied to better fit the data, and improve more quickly.

Streams have been around for sometime. The Rolling Average and Maximum have several applications, particularly in financial applications. 
-->
